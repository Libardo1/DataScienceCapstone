---
title: "CapstoneProject"
author: "Sean McCarty"
date: "Tuesday, October 28, 2014"
output: html_document
---

```{r setupEnvironment}
# http://stackoverflow.com/questions/13090838/r-markdown-avoiding-package-loading-messages
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_20')

library(Gmisc)
library(rJava)
library(RWeka)
library(R.utils)
library(tm)
library(stringi)
library(xtable)
```

```{r defineTextFileRandomSampler, echo=FALSE}
source("./sampleTextFile.R")
set.seed(1089165195)

load("./OutputData//en_US/en_USNumLines.RData")
textFilePath <- "./Data/final/en_US//"
textFileName <- "en_US.twitter.txt"

sampleTextFileUnitTest(file.path(textFilePath, textFileName),
                       num_lines,
                       200,
                       10)
```

```{r generateENRandomSample}
set.seed(1089165195)
ouputTextFileDirectory <- "./OutputData//en_US"

applyRandomSamplerToTextFiles("./Data/final//en_US",
                              10,
                              ouputTextFileDirectory)
```

```{r }
#http://stackoverflow.com/questions/12602652/how-to-count-the-number-of-sentences-in-a-text-in-r
numberOfLinesToRead <- 1000
h_conn <- file("./Data/final/en_US//en_US.twitter.txt", "r")
doc_head <- readLines(h_conn, numberOfLinesToRead)
close(h_conn)

library(openNLP)
```

```{r tokenize}
blackList <- read.csv("./Data/Terms-to-Block.csv",header=FALSE,skip=4)
blackList <- blackList[,2]
blackList <- gsub(",","",blackList)

englishText <- Corpus(DirSource(directory="./OutputData/en_US",
                                pattern="*.txt"),
                      readerControl = list(reader = readPlain,
                                           language = "en",
                                           load = TRUE))

# http://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files
# http://stackoverflow.com/questions/18153504/removing-non-english-text-from-corpus-in-r-using-tm
removeNonASCII <-
    content_transformer(function(x) iconv(x, "latin1", "ASCII", sub=""))

englishText <- tm_map(englishText, removeNonASCII)

englishText <- tm_map(englishText, stripWhitespace)

converToLowerCase <- content_transformer(function(x) tolower(x))

englishText <- tm_map(englishText, converToLowerCase)

englishText <- tm_map(englishText, removeWords, blackList)

cur_doc <- paste(as.character(englishText[[1]]),collapse="")

#englishText <- tm_map(englishText, removePunctuation)
#englishText <- tm_map(englishText, removeNumbers)
#tdc <- TermDocumentMatrix(englishText, control = list(tokenize = WordTokenizer))

# http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka
# http://stackoverflow.com/questions/8898521/finding-2-3-word-phrases-using-r-tm-package
library(tm)
library(RWeka)
data(crude)

# http://tm.r-forge.r-project.org/faq.html#Bigrams

#Tokenizer for n-grams and passed on to the term-document matrix constructor
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(c, control = list(tokenize = BigramTokenizer))
```

# http://stackoverflow.com/questions/952275/regex-group-capture-in-r
```{r tokenize}
source("./tokenizeDocument.R")

c <- Corpus(VectorSource(tokens$word))

c <- tm_map(c, removeWords, blackList)

tokens <- tokenizeDocument("./en_US.twitter100Sample.txt")
```

```{r countLineLength}
countLineLength <- function(inputTextFilePath,
                            lines_to_read,
                            num_lines) {
    total_num_lines <- num_lines[[basename(inputTextFilePath)]][1]

    # http://stackoverflow.com/questions/15532810/reading-40-gb-csv-file-into-r-using-bigmemory?lq=1
    # http://stackoverflow.com/questions/7260657/how-to-read-whitespace-delimited-strings-until-eof-in-r
    if (lines_to_read > total_num_lines) {
        lines_to_read <- total_num_lines
    }

    line_count <- vector('numeric', total_num_lines)

    h_conn <- file(inputTextFilePath, "r", blocking=FALSE)
    lines_read <- 0
    repeat {
        cur_chunk <- readLines(h_conn, lines_to_read, skipNul=TRUE)
        
        if (length(cur_chunk) == 0) {
            break
        }
        else {
            for (lineIdx in seq_len(length(cur_chunk))) {
                lines_read <- lines_read + 1
                line_count[lines_read] <- stri_length(cur_chunk[lineIdx])
            }
            print(sprintf("Read %d lines (Out of %d)", lines_read,
                                                        total_num_lines))
        }
    }
    close(h_conn)
    
    return(summary(line_count))
}

en_USNewsLineCountStats <-
    countLineLength("./Data/final/en_US//en_US.news.txt",
                    10000,
                    num_lines)

en_USBlogsLineCountStats <-
    countLineLength("./Data/final/en_US//en_US.blogs.txt",
                    10000,
                    num_lines)
``` 

```{r estimateWordPairFrequencyRatio}
blackList <- read.csv("./Data/Terms-to-Block.csv",header=FALSE,skip=4)
blackList <- blackList[,2]
blackList <- gsub(",","",blackList)

twitterData <- Corpus(DirSource(directory="./Data/final/en_US/",
                                pattern="^en_US.twitter.txt$"),
                      readerControl = list(reader = readPlain,
                                           language = "en",
                                           load = TRUE))

# http://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files
# http://stackoverflow.com/questions/18153504/removing-non-english-text-from-corpus-in-r-using-tm
removeNonASCII <-
    content_transformer(function(x) iconv(x, "latin1", "ASCII", sub=""))

twitterData <- tm_map(twitterData, removeNonASCII)

twitterData <- tm_map(twitterData, stripWhitespace)

converToLowerCase <- content_transformer(function(x) tolower(x))

twitterData <- tm_map(twitterData, converToLowerCase)

# http://stackoverflow.com/questions/17294824/counting-words-in-a-single-document-from-corpus-in-r-and-putting-it-in-dataframe

# https://www.gnu.org/software/gawk/manual/html_node/Very-Simple.html
```
