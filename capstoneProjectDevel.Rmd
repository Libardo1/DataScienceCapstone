---
title: "CapstoneProject"
author: "Sean McCarty"
date: "Tuesday, October 28, 2014"
output: html_document
---

```{r setupEnvironment}
# http://stackoverflow.com/questions/13090838/r-markdown-avoiding-package-loading-messages
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_20')

library(Gmisc)
library(rJava)
library(RWeka)
library(R.utils)
library(tm)
library(stringi)
library(xtable)
library(ggplot2)
```

```{r defineTextFileRandomSampler, echo=FALSE}
source("./sampleTextFile.R")
set.seed(1089165195)

load("./OutputData//en_US/en_USNumLines.RData")
textFilePath <- "./Data/final/en_US//"
textFileName <- "en_US.twitter.txt"

sampleTextFileUnitTest(file.path(textFilePath, textFileName),
                       num_lines,
                       200,
                       10)
```

```{r generateENRandomSample}
set.seed(1089165195)
outputTextFileDirectory <- "./OutputData//en_US/"

applyRandomSamplerToTextFiles("./Data/final//en_US/",
                              1,
                              outputTextFileDirectory)
```

```{r prototypeEDA}
blackList <- read.csv("./Data/Terms-to-Block.csv",header=FALSE,skip=4)
blackList <- blackList[,2]
blackList <- gsub(",","",blackList)

textFilePath <- outputTextFileDirectory
num_lines <- list()

for(curTextFile in dir(textFilePath, pattern = ".*1p00.*")) {
    # http://www.inside-r.org/packages/cran/R.utils/docs/countLines
    h_conn <- file(file.path(textFilePath, curTextFile), "rb")
    num_lines[[curTextFile]] <- countLines(h_conn)
    close(h_conn)
}

textFile <- "en_US.twitter1p00Percent.txt"

# http://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files
# http://stackoverflow.com/questions/18153504/removing-non-english-text-from-corpus-in-r-using-tm
removeNonASCII <-
    content_transformer(function(x) iconv(x, "latin1", "ASCII", sub=""))

converToLowerCase <- content_transformer(function(x) tolower(x))

lines_to_read = min(ceiling(num_lines[[textFile]]/10), 10000)

h_conn <- file(file.path(textFilePath, textFile), "r", blocking=FALSE)
cur_chunk <- readLines(h_conn, lines_to_read, skipNul=TRUE)
firstChunk <- TRUE

repeat {
    if (length(cur_chunk) == 0) {
        break
    }
    else {
        if (firstChunk) {
            lineCorpus <- processDocumentChunk(cur_chunk, blackList)
        }
        else {
            lineCorpus <- c(lineCorpus,
                            processDocumentChunk(cur_chunk, blackList))
        }
    }

    cur_chunk <- readLines(h_conn, lines_to_read, skipNul=TRUE)
}
close(h_conn)

processDocumentChunk <- function(cur_chunk,
                                 blackList) {
    curChunkCorpus <- Corpus(VectorSource(cur_chunk))

    curChunkCorpus <- tm_map(curChunkCorpus, removeNonASCII)

    curChunkCorpus <- tm_map(curChunkCorpus, removePunctuation)

    curChunkCorpus <- tm_map(curChunkCorpus, stripWhitespace)

    curChunkCorpus <- tm_map(curChunkCorpus, converToLowerCase)

    curChunkCorpus <- tm_map(curChunkCorpus, removeWords, blackList)    
    
    return (curChunkCorpus)
}

#http://stackoverflow.com/questions/15506118/make-dataframe-of-top-n-frequent-terms-for-multiple-corpora-using-tm-package-in
tdm <- as.matrix(TermDocumentMatrix(lineCorpus))
termFreqs <- sort(rowSums(tdm), decreasing=TRUE)
termFreqs <- 100*(termFreqs / sum(termFreqs))
termFreqs <- head(termFreqs, 20)

# http://www.r-bloggers.com/using-r-barplot-with-ggplot2/
qplot(names(termFreqs),
      termFreqs,
      main="Top 20 Terms",
      geom="bar",
      stat="identity",
      xlab="Terms",
      ylab="P(term) [%]") + coord_flip() + theme_gray(base_size=16)

#http://beyondvalence.blogspot.com/2014/01/text-mining-4-performing-term.html
```

```{r bigram}
# http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka
BigramTokenizer <- 
    function(x) {RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))}

tdmBi <- as.matrix(TermDocumentMatrix(lineCorpus, control = list(tokenize = BigramTokenizer)))

termFreqs <- sort(rowSums(tdmBi), decreasing=TRUE)
termFreqs <- termFreqs / sum(termFreqs)
termFreqs <- head(termFreqs, 20)

qplot(names(termFreqs),
      termFreqs,
      main="Top 20 Bigrams",
      geom="bar",
      stat="identity",
      xlab="Terms",
      ylab="P(Bigram)") + coord_flip() + theme_gray(base_size=16)
```

```{r }
#http://stackoverflow.com/questions/12602652/how-to-count-the-number-of-sentences-in-a-text-in-r
numberOfLinesToRead <- 1000
h_conn <- file("./Data/final/en_US//en_US.twitter.txt", "r")
doc_head <- readLines(h_conn, numberOfLinesToRead)
close(h_conn)

library(openNLP)
```

```{r tokenize}
blackList <- read.csv("./Data/Terms-to-Block.csv",header=FALSE,skip=4)
blackList <- blackList[,2]
blackList <- gsub(",","",blackList)

textCorpus <- Corpus(DirSource(directory="./OutputData/en_US",
                                pattern="*.txt"),
                      readerControl = list(reader = readPlain,
                                           language = "en",
                                           load = TRUE))

# http://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files
# http://stackoverflow.com/questions/18153504/removing-non-english-text-from-corpus-in-r-using-tm
removeNonASCII <-
    content_transformer(function(x) iconv(x, "latin1", "ASCII", sub=""))

textCorpus <- tm_map(textCorpus, removeNonASCII)

textCorpus <- tm_map(textCorpus, stripWhitespace)

converToLowerCase <- content_transformer(function(x) tolower(x))

textCorpus <- tm_map(textCorpus, converToLowerCase)

textCorpus <- tm_map(textCorpus, removeWords, blackList)

cur_doc <- paste(as.character(textCorpus[[1]]),collapse="")

#textCorpus <- tm_map(textCorpus, removePunctuation)
#textCorpus <- tm_map(textCorpus, removeNumbers)
#tdc <- TermDocumentMatrix(textCorpus, control = list(tokenize = WordTokenizer))

# http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka
# http://stackoverflow.com/questions/8898521/finding-2-3-word-phrases-using-r-tm-package
library(tm)
library(RWeka)
data(crude)

# http://tm.r-forge.r-project.org/faq.html#Bigrams

#Tokenizer for n-grams and passed on to the term-document matrix constructor
BigramTokenizer <- function(x) NGramTokenizer(x, Weka_control(min = 2, max = 2))
tdm <- TermDocumentMatrix(c, control = list(tokenize = BigramTokenizer))
```

# http://stackoverflow.com/questions/952275/regex-group-capture-in-r
```{r tokenize}
source("./tokenizeDocument.R")

c <- Corpus(VectorSource(tokens$word))

c <- tm_map(c, removeWords, blackList)

tokens <- tokenizeDocument("./en_US.twitter100Sample.txt")
```

```{r countLineLength}
countLineLength <- function(inputTextFilePath,
                            lines_to_read,
                            num_lines) {
    total_num_lines <- num_lines[[basename(inputTextFilePath)]][1]

    # http://stackoverflow.com/questions/15532810/reading-40-gb-csv-file-into-r-using-bigmemory?lq=1
    # http://stackoverflow.com/questions/7260657/how-to-read-whitespace-delimited-strings-until-eof-in-r
    if (lines_to_read > total_num_lines) {
        lines_to_read <- total_num_lines
    }

    line_count <- vector('numeric', total_num_lines)

    h_conn <- file(inputTextFilePath, "r", blocking=FALSE)
    lines_read <- 0
    repeat {
        cur_chunk <- readLines(h_conn, lines_to_read, skipNul=TRUE)
        
        if (length(cur_chunk) == 0) {
            break
        }
        else {
            for (lineIdx in seq_len(length(cur_chunk))) {
                lines_read <- lines_read + 1
                line_count[lines_read] <- stri_length(cur_chunk[lineIdx])
            }
            print(sprintf("Read %d lines (Out of %d)", lines_read,
                                                        total_num_lines))
        }
    }
    close(h_conn)
    
    return(summary(line_count))
}

en_USNewsLineCountStats <-
    countLineLength("./Data/final/en_US//en_US.news.txt",
                    10000,
                    num_lines)

en_USBlogsLineCountStats <-
    countLineLength("./Data/final/en_US//en_US.blogs.txt",
                    10000,
                    num_lines)
``` 

```{r estimateWordPairFrequencyRatio}
blackList <- read.csv("./Data/Terms-to-Block.csv",header=FALSE,skip=4)
blackList <- blackList[,2]
blackList <- gsub(",","",blackList)

twitterData <- Corpus(DirSource(directory="./Data/final/en_US/",
                                pattern="^en_US.twitter.txt$"),
                      readerControl = list(reader = readPlain,
                                           language = "en",
                                           load = TRUE))

# http://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files
# http://stackoverflow.com/questions/18153504/removing-non-english-text-from-corpus-in-r-using-tm
removeNonASCII <-
    content_transformer(function(x) iconv(x, "latin1", "ASCII", sub=""))

twitterData <- tm_map(twitterData, removeNonASCII)

twitterData <- tm_map(twitterData, stripWhitespace)

converToLowerCase <- content_transformer(function(x) tolower(x))

twitterData <- tm_map(twitterData, converToLowerCase)

# http://stackoverflow.com/questions/17294824/counting-words-in-a-single-document-from-corpus-in-r-and-putting-it-in-dataframe

# https://www.gnu.org/software/gawk/manual/html_node/Very-Simple.html
```
