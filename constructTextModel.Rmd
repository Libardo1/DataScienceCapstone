---
title: "Construct Text Model"
output: html_document
---
1.) Generates 60% / 20% / 20% split of data
2.) Analyzes training data unigram statistics
3.) Constructs Markov chain transition matrix
based on triagram statistics

```{r setupEnvironment}
# http://stackoverflow.com/questions/13090838/r-markdown-avoiding-package-loading-messages
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_20')

library(rJava)
library(RWeka)
library(R.utils)
library(stringi)
library(stringr)
library(shiny)
library(textcat)
library(tm)
library(markovchain)
library(ggplot2)
library(RColorBrewer)

source("./sampleTextFile.R")
source("./formLineCorpus.R")
source("./determineTextFileSize.R")
source("./analyzeUnigramStatistics.R")
source("./evaluateTextPredictorPerformance.R")
source("./constructTransitionMatrix.R")
```

```{r splitTextData}
set.seed(18244)
inputTextDataPath <- "./Data/final/en_US"
outputTextFileDirectory <- "./OutputData//en_US/"

if (length(dir(outputTextFileDirectory, pattern="*TrainingData.txt")) == 0) {
    load("./OutputData//en_US/en_USNumLines.RData")

    blackList <- readBlackList("./Data/Terms-to-Block.csv")
    
    splitTextDataFiles(inputTextDataPath,
                       outputTextFileDirectory,
                       num_lines)
    
    determineSplitTextDataNumLines(outputTextFileDirectory)    
}
```

```{r analyzeUnigramStatistics}
outputTextFileDirectory <- "./OutputData//en_US/"

if (!file.exists(file.path(outputTextFileDirectory,"commonTerms.RData"))) {
    load(file.path(outputTextFileDirectory,
                   "splitTextDataNumLines.RData"))
    
    analyzeTextDataUnigramStatistics(outputTextFileDirectory,
                                     ".*TrainingData.txt",
                                     num_lines,
                                     blackList)
    
    findCommonTerms(outputTextFileDirectory, 0.68)
}
```

```{r constructTransitionMatrix}
outputTextFileDirectory <- "./OutputData//en_US/"

load(file.path(outputTextFileDirectory,"commonTerms.RData"))

load(file.path(outputTextFileDirectory,
                   "splitTextDataNumLines.RData"))

blackList <- readBlackList("./Data/Terms-to-Block.csv")

constructTransitionMatrix(outputTextFileDirectory,
                          num_lines,
                          commonTerms,
                          blackList)
```

```{r analyzeTransitionMatrix}
vocabularyCounts <- loadVocabularyCounts("./OutputData//en_US")

vocabularyDistribution <- initializeVocabularyDistribution(vocabularyCounts)

ggplot(vocabularyDistribution,
       aes(x=vocabularyindex,
           y=counts,
           colour=type,
           fill=type)) + geom_point(size=2) + facet_wrap(~type) + 
    scale_color_brewer(palette="Dark2") + scale_fill_brewer(palette="Dark2") + 
    theme_gray(base_size = 14) + xlab("Vocabulary Index") + 
    ylab("Counts / Vocabulary Size")

averageTranstionMatrix <- constructAverageTransitionMatrix(vocabularyCounts)

textPredictor <- 
    new("markovchain",
        transitionMatrix=averageTranstionMatrix[["transitionMatrix"]])

save(file="./analyzeTransitionMatrix.RData", textPredictor,
                                             averageTranstionMatrix)
```

```{r evaluateTextPredictorPerformance_Part1}
load(file="./analyzeTransitionMatrix.RData")

transitionMatrix <- averageTranstionMatrix[["transitionMatrix"]]
save(file="./shinyApplication//transitionMatrix.RData", transitionMatrix)

predictorEvalParams <- list()
predictorEvalParams[["textPredictor"]] <- textPredictor
rm(textPredictor)
rm(averageTranstionMatrix)

predictorEvalParams[["textFileDirectory"]] <- "./OutputData//en_US/"

load(file.path(predictorEvalParams[["textFileDirectory"]],
               "splitTextDataNumLines.RData"))

predictorEvalParams[["num_lines"]] <- num_lines
rm(num_lines)

predictorEvalParams[["blackList"]] <- 
    readBlackList("./Data/Terms-to-Block.csv")

testDataEval <- evaluateTextPredictorPerformance(predictorEvalParams,
                                                 "*TestData.txt")
```

```{r evaluateTextPredictorPerformance_Part2}
if (!file.exists("./OutputData/en_US/vocabularyCountAdjustment.RData")) {
    vocabularyCountAdjustment <- list()
    for (curName in names(vocabularyCounts)) {
        curTestDataEval <- 
            dir("./OutputData//en_US",
                pattern=paste0(".*",curName,".*TestDataEval.RData"))
        
        load(file.path("./OutputData//en_US",curTestDataEval))
        
        filePrefix <- unlist(str_split(curTestDataEval,"\\.RData"))[1]
    
        vocabularyCountAdjustment[[filePrefix]] <- 
            sort(table(textFileEval$incorrectPredictions), decreasing=TRUE)
    }
    save(file="./OutputData//en_US/vocabularyCountAdjustment.RData",
         vocabularyCountAdjustment)    
}

vocabularyCounts <- loadVocabularyCounts("./OutputData//en_US")
load("./OutputData//en_US/vocabularyCountAdjustment.RData")

textFileEvalPrefix <- names(vocabularyCountAdjustment)

for (curDocumentType in names(vocabularyCounts)) {
    print("------------------------------------------------------------")
    print(sprintf("Updating %s's counts", curDocumentType))
    
    curField <- textFileEvalPrefix[grep(paste0(".*",curDocumentType,".*"),
                                              textFileEvalPrefix)]
    
    commonTerms <- colnames(vocabularyCounts[[curDocumentType]])
    
    curNumberTrigrams <- length(vocabularyCountAdjustment[[curField]])
    
    for (m in seq_len(curNumberTrigrams)) {
        if (m %%1000 == 0) {
            print(sprintf("    Processing trigram #%d (Out of #%d)",
                          m, curNumberTrigrams))
        }
        
        curWords <- 
            unlist(str_split(names(vocabularyCountAdjustment[[curField]][m]),
                             " "))
        
        for (n in seq(2,3)) {
            rowIdx <- which(grepl(paste0("^",curWords[n-1],"$"),
                                  commonTerms))
            
            colIdx <- which(grepl(paste0("^",curWords[n],"$"),
                                  commonTerms))
            
            vocabularyCounts[[curDocumentType]][rowIdx,colIdx] <- 
                vocabularyCounts[[curDocumentType]][rowIdx,colIdx] + 
                vocabularyCountAdjustment[[curField]][m]
        }
    }
}
save(file="./OutputData//en_US/refinedVocabularyCounts.RData",
     vocabularyCounts)

averageTranstionMatrix <- constructAverageTransitionMatrix(vocabularyCounts)

textPredictor <- 
    new("markovchain",
        transitionMatrix=averageTranstionMatrix[["transitionMatrix"]])

save(file="./analyzeRefinedTransitionMatrix.RData",
     textPredictor,
     averageTranstionMatrix)
```

```{r analyzeRefinedTransitionMatrix}
load(file="./analyzeRefinedTransitionMatrix.RData")

transitionMatrix <- averageTranstionMatrix[["transitionMatrix"]]
save(file="./shinyApplication//transitionMatrix.RData", transitionMatrix)

predictorEvalParams <- list()
predictorEvalParams[["textPredictor"]] <- textPredictor
rm(textPredictor)
rm(averageTranstionMatrix)

predictorEvalParams[["textFileDirectory"]] <- "./OutputData//en_US/"

load(file.path(predictorEvalParams[["textFileDirectory"]],
               "splitTextDataNumLines.RData"))

predictorEvalParams[["num_lines"]] <- num_lines
rm(num_lines)

predictorEvalParams[["blackList"]] <- 
    readBlackList("./Data/Terms-to-Block.csv")

validationDataEval <- evaluateTextPredictorPerformance(predictorEvalParams,
                                                       "*ValidationData.txt")
```

```{r summarizePerformance}
load(file="./OutputData//en_US/refinedVocabularyCounts.RData")

outputDataPath = "./OutputData//en_US"

trigramCount <- 0
commonTrigramCount <- 0
numCorrectPredictions <- 0
incorrectPredictions <- character()

for (curResultsFile in dir(outputDataPath,
                           pattern="*ValidationDataEval.RData")) {
    load(file.path(outputDataPath,curResultsFile))
    
    trigramCount <- trigramCount + textFileEval$trigramCount
    commonTrigramCount <- commonTrigramCount + textFileEval$commonTrigramCount
    
    numCorrectPredictions <-
        numCorrectPredictions + textFileEval$numCorrectPredictions
    
    incorrectPredictions <- append(incorrectPredictions,
                                   textFileEval$incorrectPredictions)
    
    rm(textFileEval)
}

percentTrigramsCovered <- 100*(commonTrigramCount / trigramCount)
predictorPerformance <- 100*(numCorrectPredictions / commonTrigramCount)

vocabularyDistribution <- 
    initializeVocabularyDistribution(vocabularyCounts)

ggplot(vocabularyDistribution,aes(x=zerocounts,y=counts)) + stat_binhex() +
    facet_wrap(~type) + theme_gray(base_size=16) + 
    xlab("Zero Counts (Fraction of Vocabulary)") + 
    ylab("Counts (Fraction of Voabulary)")



```