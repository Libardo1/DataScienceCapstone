---
title: "Data Science Specialization Capstone Project Exploratory Data Analysis"
author: "mspcvsp"
date: "Monday, November 10, 2014"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
    css: styles.css
---  

```{r setupEnvironment, include=FALSE}
# http://stackoverflow.com/questions/13090838/r-markdown-avoiding-package-loading-messages
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_20')

library(ggplot2)
library(Gmisc)
library(rJava)
library(RWeka)
library(R.utils)
library(stringi)
library(stringr)
library(textcat)
library(tm)
library(xtable)

if (!file.exists("./Data/final")) {
    unzip("./Data/Coursera-SwiftKey.zip", exdir="./Data")   
    
    # Profanity filter
    profanityBlackListURL <-
        paste0("http://www.frontgatemedia.com/new/wp-content/uploads/2014/03/",
               "Terms-to-Block.csv")
    
    download.file(url = profanityBlackListURL,
                  destfile="./Data/Terms-to-Block.csv")
}

blackList <- read.csv("./Data/Terms-to-Block.csv",header=FALSE,skip=4)
blackList <- blackList[,2]
blackList <- gsub(",","",blackList)

source("./determineTextFileSize.R")
source("./determineWordCount.R")
source("./formLineCorpus.R")
source("./sampleTextFile.R")
```

## Introduction

- The objective of the Data Science Capstone project is to develop an application that
predicts the next word a user will type into a mobile device
    - [Capstone Project Introduction](http://simplystatistics.org/2014/08/19/swiftkey-and-johns-hopkins-partner-for-data-science-specialization-capstone/)  
-  The purpose of this presentation is to discuss the following topics:
    - Summarize the statistics of the Capstone project text data sets
    - Describe unusual characteristics of this data  
- [An ISO 639-1 Language Code](http://www.w3schools.com/tags/ref_language_codes.asp)
    - Two-character string that refers to a language
    - Data Science Capstone project text data spans the following languages:  
        - English (EN)  
        - Finnish (FI)  
        - Germain (DE)  
        - Russian (RU)  
- The source code for this presentation has been uploaded to Github
    - [Data Science Capstone project Github repository](https://github.com/datasciencespm/DataScienceCapstone/tree/master)  

## Introduction
- This presentation will discuss the following summary statistics
    - Number of lines in each text file
    - Number of words in each text file
    - Word feature analysis

- There are three language models evaluated in this report
    - Unigram
        - Single word features
    - Bigram
        - Word pairs
    - Trigram
        - Three word segment
    - [Stanford Information Retrieval textbook language models summary](http://nlp.stanford.edu/IR-book/html/htmledition/types-of-language-models-1.html)

## Count the number of lines 
- Approach for counting the number of lines
    - Call R.utils [`countLines`](http://www.inside-r.org/packages/cran/R.utils/docs/countLines) function
    - Store number of text file lines in a list that is written to disk
    - [R script source code](https://github.com/datasciencespm/DataScienceCapstone/blob/master/determineTextFileSize.R)  
- Results presentation
    - [Format HTML table in R](http://stackoverflow.com/questions/20200082/formatting-html-table-in-r)
    - [Configure HTML table style using a Cascading Style Sheet (CSS)](http://stackoverflow.com/questions/21291762/css-how-to-define-a-class-for-table-to-control-the-style-of-table-rows)  
    - [HTML CSS Syntax](http://www.w3schools.com/css/css_table.asp)  
    - ["Stylish CSS Tables Tutorial"](http://cssmenumaker.com/blog/stylish-css-tables-tutorial)  

## Count the number of lines {.columns-2}
```{r, echo=FALSE, results='hide'}
#http://stackoverflow.com/questions/22871611/how-can-i-fix-column-breaks-in-an-rmarkdown-ioslides-presentation
baseTextFilePath <- "./Data/final"
baseOutputDataPath <- "./OutputData"

if (!file.exists(baseOutputDataPath)) {
    dir.create(baseOutputDataPath)
}

for (curLanguage in dir(baseTextFilePath)) {
    curOutputDataPath <- file.path(baseOutputDataPath, curLanguage)
    
    if (!file.exists(curOutputDataPath)) {
        dir.create(curOutputDataPath)
    }

    determineTextFileSize(file.path(baseTextFilePath, curLanguage),
                          curOutputDataPath)
    
    load(file.path(baseOutputDataPath,
                   curLanguage,
                   paste0(curLanguage,"NumLines.RData")))
        
    determineWordCount(file.path(baseTextFilePath, curLanguage),
                       curOutputDataPath,
                       num_lines)
}
```

- German text documents # of lines:
```{r loadDENumberOfLines, echo=FALSE, results='asis'}
#http://stackoverflow.com/questions/20200082/formatting-html-table-in-r
#http://stackoverflow.com/questions/21291762/css-how-to-define-a-class-for-table-to-control-the-style-of-table-rows
#http://www.w3schools.com/css/css_table.asp
#http://cssmenumaker.com/blog/stylish-css-tables-tutorial
load("./OutputData//de_DE/de_DENumLines.RData")
num_lines = t(as.data.frame(num_lines))
colnames(num_lines) <- c("# of lines")
print(xtable(t(num_lines)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_lines)
```
</br>
- English text documents # of lines:  
```{r loadENNumberOfLines, echo=FALSE, results='asis'}
load("./OutputData//en_US/en_USNumLines.RData")
num_lines = t(as.data.frame(num_lines))
colnames(num_lines) <- c("# of lines")
print(xtable(t(num_lines)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_lines)
```
</br>
- Finnish text documents # of lines:
```{r loadFINumberOfLines, echo=FALSE, results='asis'}
load("./OutputData//fi_FI/fi_FINumLines.RData")
num_lines = t(as.data.frame(num_lines))
colnames(num_lines) <- c("# of lines")
print(xtable(t(num_lines)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_lines)
```
</br>
- Russian text documents # of lines:
```{r loadRUNumberOfLines, echo=FALSE, results='asis'}
load("./OutputData//ru_RU/ru_RUNumLines.RData")
num_lines = t(as.data.frame(num_lines))
colnames(num_lines) <- c("# of lines")
print(xtable(t(num_lines)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_lines)
```

## Count the number of words
- Approach for counting the number of lines
    - [Read the text file into memory in chunks](http://stackoverflow.com/questions/15532810/reading-40-gb-csv-file-into-r-using-bigmemory?lq=1)  
    - [Remove non-ASCII character from text](http://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files)
    - [Count the number of words using the stringi R package `stri_stats_latex` function](http://www.r-bloggers.com/counting-the-number-of-words-in-a-latex-file-with-stringi/)  
    - Store the text file word count in a list that is written to disk
    - Present results in the same format at the number of lines / text file
    - [R script source code](https://github.com/datasciencespm/DataScienceCapstone/blob/master/determineWordCount.R)

## Count the number of words {.columns-2}
```{r countNumberOfWords, echo=FALSE, results='hide'}
#http://stackoverflow.com/questions/22871611/how-can-i-fix-column-breaks-in-an-rmarkdown-ioslides-presentation
if (!file.exists(baseOutputDataPath)) {
    dir.create(baseOutputDataPath)
}

for (curLanguage in dir(baseTextFilePath)) {
    curOutputDataPath <- file.path(baseOutputDataPath, curLanguage)
    
    load(file.path(baseOutputDataPath,
                   curLanguage,
                   paste0(curLanguage,"NumLines.RData")))
        
    determineWordCount(file.path(baseTextFilePath, curLanguage),
                       curOutputDataPath,
                       num_lines)
}
```

- German text documents # of words [millions]:
```{r loadDENumberOfWords, echo=FALSE, results='asis'}
load("./OutputData//de_DE/de_DENumWords.RData")
num_words = t(as.data.frame(num_words)) / 1E6
colnames(num_words) <- "# of words [millions]"
print(xtable(t(num_words)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_words)
```
</br>
- English text documents # of words [millions]:  
```{r loadENNumberOfWords, echo=FALSE, results='asis'}
load("./OutputData//en_US/en_USNumWords.RData")
num_words = t(as.data.frame(num_words)) / 1E6
colnames(num_words) <- "# of words [millions]"
print(xtable(t(num_words)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_words)
```
</br>
- Finnish text documents # of words [millions]:
```{r loadFINumberOfWords, echo=FALSE, results='asis'}
load("./OutputData//fi_FI/fi_FINumWords.RData")
num_words = t(as.data.frame(num_words)) / 1E6
colnames(num_words) <- "# of words [millions]"
print(xtable(t(num_words)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_words)
```
</br>
- Russian text documents # of words:
```{r loadRUNumberOfWords, echo=FALSE, results='asis'}
load("./OutputData//ru_RU/ru_RUNumWords.RData")
num_words = t(as.data.frame(num_words)) / 1E6
colnames(num_words) <- "# of words [millions]"
print(xtable(t(num_words)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_words)
```

## Unique Word Cumulative Distribution Function    
```{r initializeLineCorpusConstruction, echo=FALSE, results='hide'}
set.seed(1089165195)
outputTextFileDirectory <- "./OutputData//en_US/"
percentageToSample <- 1
```

- This plot illustrates the number of words required to model 50% and 90% of words
in a `r percentageToSample`% sample of the english language text files

```{r constructLineCorpus, echo=FALSE, results='hide', fig.height=4.5, fig.width=7, fig.align='center'}
samplingStr <- initializeSamplingString(percentageToSample)

sampledTextFileRegex <- initializeSampledTextFileRegex(percentageToSample)

if (length(dir(outputTextFileDirectory,
               pattern=sampledTextFileRegex)) == 0){
    applyRandomSamplerToTextFiles("./Data/final//en_US/",
                                  percentageToSample,
                                  outputTextFileDirectory,
                                  TRUE) 
}

languageId <- basename(outputTextFileDirectory)

lineCorpusFile <- file.path(outputTextFileDirectory,
                            pattern=paste0(languageId,
                                           samplingStr,
                                           "LineCorpus.RData"))

if (!file.exists(lineCorpusFile)) {
    sampledTextFileSizePath <- 
        determineSampledTextFileSize(outputTextFileDirectory,
                                     percentageToSample)
    load(outputFilePath)

    lineCorpus <- list()    
    for (curTextFile in dir(outputTextFileDirectory,
                            pattern=sampledTextFileRegex)) {
        lineCorpus[[curTextFile]] <- 
            formLineCorpus(outputTextFileDirectory,
                           curTextFile,
                           "english",
                           sampledDocNumLines,
                           blackList,
                           TRUE)
    }
    save(file=lineCorpusFile, lineCorpus)    
}
load(lineCorpusFile)

sampledTextFiles <- names(lineCorpus)
for (n in seq_len(length(sampledTextFiles))) {
    if (n == 1){
        combinedCorpus <- lineCorpus[[sampledTextFiles[n]]]
    } else {
        combinedCorpus <- c(combinedCorpus,
                            lineCorpus[[sampledTextFiles[n]]])
    }
}
rm(lineCorpus)

tdm <- as.matrix(TermDocumentMatrix(combinedCorpus))
termFreqs <- sort(rowSums(tdm), decreasing=TRUE)
termPDF <- termFreqs / sum(termFreqs)
termCDF <- cumsum(termPDF)

fiftyPercentIdx <- which(termCDF >= 0.50)[1]
sixtyFivePercentIdx <- which(termCDF >= 0.65)[1]
eightyFivePercentIdx <- which(termCDF >= 0.85)[1]
ninetyPercentIdx <- which(termCDF >= 0.90)[1]

plot(termCDF,
     xlab="# of unique words",
     ylab="Unique Words Cumulative Distribution Function",
     panel.first = grid(), pch=19, col="darkblue")

lines(fiftyPercentIdx*rep(1,6),seq(from=0,to=0.5,by=0.1),
      col="firebrick",lty=2,lwd=2)
lines(seq(from=1,to=fiftyPercentIdx,by=1),0.5*rep(1,fiftyPercentIdx),
      col="firebrick",lty=2,lwd=2)
arrows(sixtyFivePercentIdx,0.42,fiftyPercentIdx,0.5,
       lwd=2,length=0.15,col="firebrick")
text(sixtyFivePercentIdx,0.4,sprintf("# of words: %d", fiftyPercentIdx),pos=4,
     col="firebrick")

lines(ninetyPercentIdx*rep(1,10),seq(from=0,to=0.9,by=0.1),
      col="springgreen4",lty=2,lwd=2)
lines(seq(from=1,to=ninetyPercentIdx,by=1),0.9*rep(1,ninetyPercentIdx),
      col="springgreen4",lty=2,lwd=2)
arrows(eightyFivePercentIdx,0.72,ninetyPercentIdx,0.9,
       lwd=2,length=0.15,col="springgreen4")
text(eightyFivePercentIdx,0.72,sprintf("# of words: %d", ninetyPercentIdx),
     pos=1,col="springgreen4")
```

## Unigram Distribution  
- This bar chart illustrates the top twenty words (i.e. 'unigrams') in a 
`r percentageToSample`% sample of the english language text files  

```{r plotTopTwentyWord, echo=FALSE, results='hide', fig.height=4.5, fig.width=7, fig.align='center'}
termFreqs <- 100*(termFreqs / sum(termFreqs))
termFreqsTopTwenty <- head(termFreqs, 20)

cumualtiveSum <- cumsum(termFreqsTopTwenty)

# http://www.r-bloggers.com/using-r-barplot-with-ggplot2/
qplot(names(termFreqsTopTwenty),
      termFreqsTopTwenty,
      main="Top 20 English Language Words",
      geom="bar",
      stat="identity",
      xlab="Terms",
      ylab="P(term) [%]") + coord_flip() + theme_gray(base_size=16)

rm(tdm)
rm(termFreqs)
```

## Bigram Distribution  
- This bar chart illustrates the top twenty word pairs (i.e. 'bigrams') in a 
`r percentageToSample`% sample of the english language text files  

```{r plotTopTwentyBigrams, echo=FALSE, results='hide', fig.height=4.5, fig.width=7, fig.align='center'}
BigramTokenizer <- function(x) {
    RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))
}

tdmBi <- 
    as.matrix(TermDocumentMatrix(combinedCorpus,
                                 control = list(tokenize = BigramTokenizer)))

bigramFreqs <- sort(rowSums(tdmBi), decreasing=TRUE)
bigramFreqs <- 100*bigramFreqs / sum(bigramFreqs)
bigramFreqsTopTwenty <- head(bigramFreqs, 20)
bigramFreqsBottomTwenty <- tail(bigramFreqs, 20)

qplot(names(bigramFreqsTopTwenty),
      bigramFreqsTopTwenty,
      main="Top 20 Bigrams",
      geom="bar",
      stat="identity",
      xlab="Terms",
      ylab="P(Bigram)") + coord_flip() + theme_gray(base_size=16)

rm(tdmBi)
rm(bigramFreqs)
```
  
## Bigram Distribution
- This bar chart illustrates the bottom twenty word pairs (i.e. 'bigrams') in a 
`r percentageToSample`% sample of the english language text files  
```{r plotBottomTwentyBigrams, echo=FALSE, results='hide', fig.height=4.5, fig.width=7, fig.align='center'}
qplot(names(bigramFreqsBottomTwenty),
      bigramFreqsBottomTwenty,
      main="Bottom 20 Bigrams",
      geom="bar",
      stat="identity",
      xlab="Terms",
      ylab="P(Bigram)") + coord_flip() + theme_gray(base_size=16)
```
  
## Trigram Distribution  
- This bar chart illustrates the top twenty word triplets (i.e. 'trigrams') in a 
`r percentageToSample`% sample of the english language text files  

```{r plotTopTwentyTrigrams, echo=FALSE, results='hide', fig.height=4.5, fig.width=7, fig.align='center'}
TrigramTokenizer <- function(x) {
    RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))
}

tdmTri <- 
    as.matrix(TermDocumentMatrix(combinedCorpus,
                                 control = list(tokenize = TrigramTokenizer)))

trigramFreqs <- sort(rowSums(tdmTri), decreasing=TRUE)
trigramFreqs <- 100*trigramFreqs / sum(trigramFreqs)
trigramFreqs <- head(trigramFreqs, 20)

qplot(names(trigramFreqs),
      trigramFreqs,
      main="Top 20 Trigrams",
      geom="bar",
      stat="identity",
      xlab="Terms",
      ylab="P(Trigram)") + coord_flip() + theme_gray(base_size=16)
```
  