---
title: "JHU Data Science Specialization Capstone Project Exploratory Data Analysis"
author: "mspcvsp"
date: "November 16, 2014"
output:
  ioslides_presentation:
    widescreen: true
    smaller: true
    css: styles.css
---  

```{r setupEnvironment, include=FALSE}
# http://stackoverflow.com/questions/13090838/r-markdown-avoiding-package-loading-messages
Sys.setenv(JAVA_HOME='C:\\Program Files\\Java\\jre1.8.0_20')

library(ggplot2)
library(Gmisc)
library(rJava)
library(RWeka)
library(R.utils)
library(stringi)
library(stringr)
library(textcat)
library(tm)
library(xtable)

if (!file.exists("./Data/final")) {
    unzip("./Data/Coursera-SwiftKey.zip", exdir="./Data")   
    
    # Profanity filter
    profanityBlackListURL <-
        paste0("http://www.c.com/new/wp-content/uploads/2014/03/",
               "Terms-to-Block.csv")
    
    download.file(url = profanityBlackListURL,
                  destfile="./Data/Terms-to-Block.csv")
}

blackList <- read.csv("./Data/Terms-to-Block.csv",header=FALSE,skip=4)
blackList <- blackList[,2]
blackList <- gsub(",","",blackList)

source("./determineTextFileSize.R")
source("./determineWordCount.R")
source("./formLineCorpus.R")
source("./sampleTextFile.R")
```

## Introduction

- The objective of the Data Science Capstone project is to develop an application that
predicts the next word a user will type into a mobile device
    - [Capstone project introduction](http://simplystatistics.org/2014/08/19/swiftkey-and-johns-hopkins-partner-for-data-science-specialization-capstone/)  
-  The purpose of this presentation is to discuss the following topics:
    - Summarize the statistics of the Capstone project text data sets
    - Describe unusual characteristics of this data  
    - Describe my plan for developing a prediction algorithm & [Shiny](http://shiny.rstudio.com/) application  
- [An ISO 639-1 Language Code](http://www.w3schools.com/tags/ref_language_codes.asp)
    - Two-character string that refers to a language
    - Data Science Capstone project text data spans the following languages:  
        - English (EN)  
        - Finnish (FI)  
        - German (DE)  
        - Russian (RU)  
- The source code for this presentation has been uploaded to Github
    - [Data Science Capstone project Github repository](https://github.com/datasciencespm/DataScienceCapstone/tree/master)  

## Introduction
- This presentation will discuss the following summary statistics
    - Number of lines in each text file
    - Number of words in each text file

- There are three language models evaluated in this report
    - Unigram
        - Single word features
    - Bigram
        - Word pairs
    - Trigram
        - Three word segment
    - [Stanford Information Retrieval textbook language models summary](http://nlp.stanford.edu/IR-book/html/htmledition/types-of-language-models-1.html)

## Count the number of lines 
- Approach for counting the number of lines
    - Call R.utils [`countLines`](http://www.inside-r.org/packages/cran/R.utils/docs/countLines) function
    - Store number of text file lines in a list that is written to disk
    - [R script source code](https://github.com/datasciencespm/DataScienceCapstone/blob/master/determineTextFileSize.R)  
- Results presentation
    - [Format HTML table in R](http://stackoverflow.com/questions/20200082/formatting-html-table-in-r)
    - [Configure HTML table style using a Cascading Style Sheet (CSS)](http://stackoverflow.com/questions/21291762/css-how-to-define-a-class-for-table-to-control-the-style-of-table-rows)  
    - [HTML CSS Syntax](http://www.w3schools.com/css/css_table.asp)  
    - ["Stylish CSS Tables Tutorial"](http://cssmenumaker.com/blog/stylish-css-tables-tutorial)  

## Count the number of lines {.columns-2}
```{r, echo=FALSE, results='hide'}
#http://stackoverflow.com/questions/22871611/how-can-i-fix-column-breaks-in-an-rmarkdown-ioslides-presentation
baseTextFilePath <- "./Data/final"
baseOutputDataPath <- "./OutputData"

if (!file.exists(baseOutputDataPath)) {
    dir.create(baseOutputDataPath)
}

for (curLanguage in dir(baseTextFilePath)) {
    curOutputDataPath <- file.path(baseOutputDataPath, curLanguage)
    
    if (!file.exists(curOutputDataPath)) {
        dir.create(curOutputDataPath)
    }

    determineTextFileSize(file.path(baseTextFilePath, curLanguage),
                          curOutputDataPath)
    
    load(file.path(baseOutputDataPath,
                   curLanguage,
                   paste0(curLanguage,"NumLines.RData")))
        
    determineWordCount(file.path(baseTextFilePath, curLanguage),
                       curOutputDataPath,
                       num_lines)
}
```

- German text documents # of lines:
```{r loadDENumberOfLines, echo=FALSE, results='asis'}
#http://stackoverflow.com/questions/20200082/formatting-html-table-in-r
#http://stackoverflow.com/questions/21291762/css-how-to-define-a-class-for-table-to-control-the-style-of-table-rows
#http://www.w3schools.com/css/css_table.asp
#http://cssmenumaker.com/blog/stylish-css-tables-tutorial
load("./OutputData//de_DE/de_DENumLines.RData")
num_lines = t(as.data.frame(num_lines))
colnames(num_lines) <- c("# of lines")
print(xtable(t(num_lines)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_lines)
```
</br>
- English text documents # of lines:  
```{r loadENNumberOfLines, echo=FALSE, results='asis'}
load("./OutputData//en_US/en_USNumLines.RData")
num_lines = t(as.data.frame(num_lines))
colnames(num_lines) <- c("# of lines")
print(xtable(t(num_lines)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_lines)
```
</br>
- Finnish text documents # of lines:
```{r loadFINumberOfLines, echo=FALSE, results='asis'}
load("./OutputData//fi_FI/fi_FINumLines.RData")
num_lines = t(as.data.frame(num_lines))
colnames(num_lines) <- c("# of lines")
print(xtable(t(num_lines)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_lines)
```
</br>
- Russian text documents # of lines:
```{r loadRUNumberOfLines, echo=FALSE, results='asis'}
load("./OutputData//ru_RU/ru_RUNumLines.RData")
num_lines = t(as.data.frame(num_lines))
colnames(num_lines) <- c("# of lines")
print(xtable(t(num_lines)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_lines)
```

## Count the number of words
- Approach for counting the number of lines
    - [Read the text file into memory in chunks](http://stackoverflow.com/questions/15532810/reading-40-gb-csv-file-into-r-using-bigmemory?lq=1)  
    - [Remove non-ASCII character from text](http://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files)
    - [Count the number of words using the stringi R package `stri_stats_latex` function](http://www.r-bloggers.com/counting-the-number-of-words-in-a-latex-file-with-stringi/)  
    - Store the text file word count in a list that is written to disk
    - Present results in the same format at the number of lines / text file
    - [R script source code](https://github.com/datasciencespm/DataScienceCapstone/blob/master/determineWordCount.R)

## Count the number of words {.columns-2}
```{r countNumberOfWords, echo=FALSE, results='hide'}
#http://stackoverflow.com/questions/22871611/how-can-i-fix-column-breaks-in-an-rmarkdown-ioslides-presentation
if (!file.exists(baseOutputDataPath)) {
    dir.create(baseOutputDataPath)
}

for (curLanguage in dir(baseTextFilePath)) {
    curOutputDataPath <- file.path(baseOutputDataPath, curLanguage)
    
    load(file.path(baseOutputDataPath,
                   curLanguage,
                   paste0(curLanguage,"NumLines.RData")))
        
    determineWordCount(file.path(baseTextFilePath, curLanguage),
                       curOutputDataPath,
                       num_lines)
}
```

- German text documents # of words [millions]:
```{r loadDENumberOfWords, echo=FALSE, results='asis'}
load("./OutputData//de_DE/de_DENumWords.RData")
num_words = t(as.data.frame(num_words)) / 1E6
colnames(num_words) <- "# of words [millions]"
print(xtable(t(num_words)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_words)
```
</br>
- English text documents # of words [millions]:  
```{r loadENNumberOfWords, echo=FALSE, results='asis'}
load("./OutputData//en_US/en_USNumWords.RData")
num_words = t(as.data.frame(num_words)) / 1E6
colnames(num_words) <- "# of words [millions]"
print(xtable(t(num_words)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_words)
```
</br>
- Finnish text documents # of words [millions]:
```{r loadFINumberOfWords, echo=FALSE, results='asis'}
load("./OutputData//fi_FI/fi_FINumWords.RData")
num_words = t(as.data.frame(num_words)) / 1E6
colnames(num_words) <- "# of words [millions]"
print(xtable(t(num_words)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_words)
```
</br>
- Russian text documents # of words:
```{r loadRUNumberOfWords, echo=FALSE, results='asis'}
load("./OutputData//ru_RU/ru_RUNumWords.RData")
num_words = t(as.data.frame(num_words)) / 1E6
colnames(num_words) <- "# of words [millions]"
print(xtable(t(num_words)),
      type="html",
      html.table.attributes="class='table-bordered'")
rm(num_words)
```

## Word Feature Characterization
- Sample text documents
    - [Read large text file into memory](http://stackoverflow.com/questions/15532810/reading-40-gb-csv-file-into-r-using-bigmemory?lq=1)
- Form a collection of documents (i.e. `corpus`) from text file lines
    - Input is a randomly sampled percentage of the original text files
        - No correspondence between lines  
    - Read text data N lines at a time
        - Avoids running out of memory
    - Remove non-ASCII characters from text file lines
        - [Remove non-ASCII characters from text data](http://stackoverflow.com/questions/9934856/removing-non-ascii-characters-from-data-files)  
        - [Remove non-ASCII characters from corpus](http://stackoverflow.com/questions/18153504/removing-non-english-text-from-corpus-in-r-using-tm)  
    - Detect and remove non-english language text
        - [Segment text language using the textcat R package](http://stackoverflow.com/questions/24099098/language-detection-in-r-with-the-textcat-package-how-to-restrict-to-a-few-lang)  
        
## Word Feature Characterization  
```{r initializeLineCorpusConstruction, echo=FALSE, results='hide'}
set.seed(1089165195)
outputTextFileDirectory <- "./OutputData//en_US/"
percentageToSample <- 1
```
- Form a collection of documents (i.e. `Corpus`) from text file lines (cont.)
    - Apply the following transformations to the text data
         - Remove punctuation 
         - Remove [whitespace](http://www.w3.org/TR/html401/struct/text.html)
         - Convert text to lower case
         - Remove profane words contained in a ["black list"](http://www.frontgatemedia.com/a-list-of-723-bad-words-to-blacklist-and-how-to-use-facebooks-moderation-tool/)
- Word feature analysis approach  
    - Analyze a `r percentageToSample`% sample of the english language text files  
    - Unique word Cumulative Distribution Function (CDF)
    - Top 20 (i.e. highest frequency of occurrence) words 
    - Top 20 word pairs (i.e. bigrams)
    - Bottom 20 (i.e. least likely) bigrams
    - Top 20 word triplets (i.e. trigrams)
    - Bottom 20 trigrams  

## Unique Word Cumulative Distribution Function  
- This plot illustrates the number of words required to model 50% and 90% of 
unique words in a `r percentageToSample`% sample of the english language text 
files

```{r constructLineCorpus, echo=FALSE, results='hide', cache=TRUE}
samplingStr <- initializeSamplingString(percentageToSample)

sampledTextFileRegex <- initializeSampledTextFileRegex(percentageToSample)

if (length(dir(outputTextFileDirectory,
               pattern=sampledTextFileRegex)) == 0){
    applyRandomSamplerToTextFiles("./Data/final//en_US/",
                                  percentageToSample,
                                  outputTextFileDirectory,
                                  TRUE) 
}

languageId <- basename(outputTextFileDirectory)

lineCorpusFile <- file.path(outputTextFileDirectory,
                            pattern=paste0(languageId,
                                           samplingStr,
                                           "LineCorpus.RData"))

if (!file.exists(lineCorpusFile)) {
    sampledTextFileSizePath <- 
        determineSampledTextFileSize(outputTextFileDirectory,
                                     percentageToSample)
    load(sampledTextFileSizePath)

    lineCorpus <- list()    
    for (curTextFile in dir(outputTextFileDirectory,
                            pattern=sampledTextFileRegex)) {
        lineCorpus[[curTextFile]] <- 
            formLineCorpus(outputTextFileDirectory,
                           curTextFile,
                           "english",
                           sampledDocNumLines,
                           blackList,
                           TRUE)
    }
    save(file=lineCorpusFile, lineCorpus)    
}
load(lineCorpusFile)

sampledTextFiles <- names(lineCorpus)
for (n in seq_len(length(sampledTextFiles))) {
    if (n == 1){
        combinedCorpus <- lineCorpus[[sampledTextFiles[n]]]
    } else {
        combinedCorpus <- c(combinedCorpus,
                            lineCorpus[[sampledTextFiles[n]]])
    }
}
rm(lineCorpus)

tdm <- as.matrix(TermDocumentMatrix(combinedCorpus))
termFreqs <- sort(rowSums(tdm), decreasing=TRUE)
termPDF <- termFreqs / sum(termFreqs)
termCDF <- cumsum(termPDF)
```

```{r plotUniqueWordsCDF,echo=FALSE, results='hide', fig.height=4.5, fig.width=7, fig.align='center'}
fiftyPercentIdx <- which(termCDF >= 0.50)[1]
sixtyFivePercentIdx <- which(termCDF >= 0.65)[1]
eightyFivePercentIdx <- which(termCDF >= 0.85)[1]
ninetyPercentIdx <- which(termCDF >= 0.90)[1]

plot(termCDF,
     xlab="# of unique words",
     ylab="Unique Words Cumulative Distribution Function",
     panel.first = grid(), pch=19, col="darkblue")

lines(fiftyPercentIdx*rep(1,6),seq(from=0,to=0.5,by=0.1),
      col="firebrick",lty=2,lwd=2)
lines(seq(from=1,to=fiftyPercentIdx,by=1),0.5*rep(1,fiftyPercentIdx),
      col="firebrick",lty=2,lwd=2)
arrows(sixtyFivePercentIdx,0.42,fiftyPercentIdx,0.5,
       lwd=2,length=0.15,col="firebrick")
text(sixtyFivePercentIdx,0.4,sprintf("# of words: %d", fiftyPercentIdx),pos=4,
     col="firebrick")

lines(ninetyPercentIdx*rep(1,10),seq(from=0,to=0.9,by=0.1),
      col="springgreen4",lty=2,lwd=2)
lines(seq(from=1,to=ninetyPercentIdx,by=1),0.9*rep(1,ninetyPercentIdx),
      col="springgreen4",lty=2,lwd=2)
arrows(eightyFivePercentIdx,0.72,ninetyPercentIdx,0.9,
       lwd=2,length=0.15,col="springgreen4")
text(eightyFivePercentIdx,0.72,sprintf("# of words: %d", ninetyPercentIdx),
     pos=1,col="springgreen4")
```

## Unigram Distribution  
- This bar chart illustrates the top twenty words (i.e. 'unigrams') in a 
`r percentageToSample`% sample of the english language text files  

```{r plotTopTwentyWord, echo=FALSE, results='hide', fig.height=4.5, fig.width=7, fig.align='center'}
termFreqs <- 100*(termFreqs / sum(termFreqs))
termFreqsTopTwenty <- head(termFreqs, 20)

cumualtiveSum <- cumsum(termFreqsTopTwenty)

# http://www.r-bloggers.com/using-r-barplot-with-ggplot2/
qplot(names(termFreqsTopTwenty),
      termFreqsTopTwenty,
      main="Top 20 English Language Words",
      geom="bar",
      stat="identity",
      xlab="Word",
      ylab="P(Word) [%]") + coord_flip() + theme_gray(base_size=14) +
      theme(axis.text.x = element_text(angle = 90, hjust = 1))

rm(tdm)
rm(termFreqs)
```

## Bigram Distribution  
- This bar chart illustrates the top twenty word pairs (i.e. 'bigrams') in a 
`r percentageToSample`% sample of the english language text files  

```{r constructBigramTDM, echo=FALSE, results='hide', cache=TRUE}
BigramTokenizer <- function(x) {
    RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 2, max = 2))
}

tdmBi <- 
    as.matrix(TermDocumentMatrix(combinedCorpus,
                                 control = list(tokenize = BigramTokenizer)))
```

```{r plotTopTwentyBigrams, echo=FALSE, results='hide', fig.height=4, fig.width=8, fig.align='center'}
bigramFreqs <- sort(rowSums(tdmBi), decreasing=TRUE)
bigramFreqs <- 100*bigramFreqs / sum(bigramFreqs)
bigramFreqsTopTwenty <- head(bigramFreqs, 20)
bigramFreqsBottomTwenty <- tail(bigramFreqs, 20)

qplot(names(bigramFreqsTopTwenty),
      bigramFreqsTopTwenty,
      main="Top 20 English Language Bigrams",
      geom="bar",
      stat="identity",
      xlab="Bigram",
      ylab="P(Bigram)") + coord_flip() + theme_gray(base_size=14) +
      theme(axis.text.x = element_text(angle = 90, hjust = 1))

rm(tdmBi)
rm(bigramFreqs)
```
  
## Bigram Distribution
- This bar chart illustrates the bottom twenty word pairs (i.e. 'bigrams') in a 
`r percentageToSample`% sample of the english language text files  
    - Unexpected result suggests bigrams that include the word `zombie` occur 
    with a relatively low frequency  
```{r plotBottomTwentyBigrams, echo=FALSE, results='hide', fig.height=4, fig.width=8, fig.align='center'}
qplot(names(bigramFreqsBottomTwenty),
      bigramFreqsBottomTwenty,
      main="Bottom 20 English Language Bigrams",
      geom="bar",
      stat="identity",
      xlab="Bigram",
      ylab="P(Bigram)") + coord_flip() + theme_gray(base_size=14) +
      theme(axis.text.x = element_text(angle = 90, hjust = 1))
```
  
## Trigram Distribution  
- This bar chart illustrates the top twenty word triplets (i.e. 'trigrams') in a 
`r percentageToSample`% sample of the english language text files  

```{r computeTrigramTDM, echo=FALSE, results='hide'}
TrigramTokenizer <- function(x) {
    RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))
}

tdmTri <- 
    as.matrix(TermDocumentMatrix(combinedCorpus,
                                 control = list(tokenize = TrigramTokenizer)))
```

```{r plotTopTwentyTrigrams, echo=FALSE, results='hide', fig.height=4, fig.width=8, fig.align='center'}
trigramFreqs <- sort(rowSums(tdmTri), decreasing=TRUE)
trigramFreqs <- 100*trigramFreqs / sum(trigramFreqs)
trigramFreqsTopTwenty <- head(trigramFreqs, 20)
trigramFreqsBottomTwenty <- tail(trigramFreqs, 20)
rm(trigramFreqs)

qplot(names(trigramFreqsTopTwenty),
      trigramFreqsTopTwenty,
      main="Top 20 English Language Trigrams",
      geom="bar",
      stat="identity",
      xlab="Trigram",
      ylab="P(Trigram)") + coord_flip() + theme_gray(base_size=14) +
      theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Trigram Distribution  
- This bar chart illustrates the bottom twenty word triplets (i.e. 'trigrams') in a 
`r percentageToSample`% sample of the english language text files  
    - Unexpected result suggests trigrams that include the word `zombie` occur
    with a relatively low frequency

```{r plotBottomsTwentyTrigrams, echo=FALSE, results='hide', fig.height=4, fig.width=8, fig.align='center'}
qplot(names(trigramFreqsBottomTwenty),
      trigramFreqsBottomTwenty,
      main="Bottom 20 English Language Trigrams",
      geom="bar",
      stat="identity",
      xlab="Trigram",
      ylab="P(Trigram)") + coord_flip() + theme_gray(base_size=14) +
      theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

## Prediction Algorithm & Shiny Application Development Plan  
- Start with building a simple n-gram prediction model
    - [Carmignani, N. "Predicting Words and Sentences using Statistical Models"](http://medialab.di.unipi.it/web/Language+Intelligence/Prediction.pdf)  
    - [Hillard, D. S. Petersen & M. Ostendorf. "N-gram Language Modeling Tutorial"](http://ssli.ee.washington.edu/WS07/notes/ngrams.pdf)  
    - [Bickel, S., P. Haider, and T. Scheffer. "Predicting Sentences using N-Gram Language Models"](http://www.cs.uni-potsdam.de/ml/publications/emnlp2005.pdf)
- Evaluate Markov chains language model implementation
    - [Antonov, A. "Markov chains n-gram model implementation"](http://mathematicaforprediction.wordpress.com/2014/01/25/markov-chains-n-gram-model-implementation/)  
- Focus on minimizing the prediction application memory requirement
    - [Cohen P. & C. A. Sutton. "Very Predictive Ngrams for Space-Limited Probabilistic Models"](http://homepages.inf.ed.ac.uk/csutton/publications/vpr.pdf)  

## Summary  
- This presentation discussed the following summary statistics
    - Number of lines in each text file
    - Number of words in each text file  

- Three language models were evaluated in this report
    - Unigram
        - Single word features
    - Bigram
        - Word pairs
    - Trigram
        - Three word segment

- Prediction algorithm & [Shiny](http://shiny.rstudio.com/) application development plan was presented  

## Word Feature Analysis Technical References 
- [Text arrow graph annotation](http://rpsychologist.com/creating-a-typical-textbook-illustration-of-statistical-power-using-either-ggplot-or-base-graphics)  
- [Rotate ggplot2 axis label](http://stackoverflow.com/questions/1330989/rotating-and-spacing-axis-labels-in-ggplot2)
- [Construct data frame that contains the most frequency words in a corpus](http://stackoverflow.com/questions/15506118/make-dataframe-of-top-n-frequent-terms-for-multiple-corpora-using-tm-package-in)  
- [Term Frequency visualization](http://beyondvalence.blogspot.com/2014/01/text-mining-4-performing-term.html)  
    - [Term Document Matrix (TDM)](http://beyondvalence.blogspot.com/2014/01/text-mining-3-stemming-text-and.html)  
- [Rweka R package syntax for computing a bigram TDM](http://stackoverflow.com/questions/17703553/bigrams-instead-of-single-words-in-termdocument-matrix-using-r-and-rweka)  
- [Rweka R package syntax for computing a trigram TDM](http://stackoverflow.com/questions/19615181/finding-ngrams-in-r-and-comparing-ngrams-across-corpora)  
- Word feature analysis R scripts
    - [Sample text file](https://github.com/datasciencespm/DataScienceCapstone/blob/master/sampleTextFile.R)
    - [Form text file line corpus](https://github.com/datasciencespm/DataScienceCapstone/blob/master/formLineCorpus.R)  
