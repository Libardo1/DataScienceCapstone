---
title: "MarkovChain"
output: html_document
---

```{r setupEnvironment}
library(ggplot2)
library(Gmisc)
library(rJava)
library(RWeka)
library(R.utils)
library(stringi)
library(stringr)
library(textcat)
library(tm)
library(xtable)
library(markovchain)

source("./determineTextFileSize.R")
source("./determineWordCount.R")
source("./formLineCorpus.R")
source("./sampleTextFile.R")
```

```{r updateLineCorpus}
updateLineCorpus <- TRUE
if (updateLineCorpus)
    blackListFile <- "./Data/Terms-to-Block.csv"

    readBlackList <- function(blackListFile) {
        blackList <- read.csv(blackListFile,header=FALSE,skip=4)
        blackList <- blackList[,2]
        blackList <- gsub(",","",blackList)
        blackList <- gsub(" ","",blackList)
        blackList <- gsub("[0-9]+","",blackList)
        blackList <- gsub("[\\.-]","",blackList)
        blackList <- blackList[!grepl("^a$",blackList)]
        blackList <- unique(blackList[blackList != ""])
        return(blackList)
    }

    blackList <- readBlackList(blackListFile)

    outputTextFileDirectory <- "./OutputData//en_US/"
    percentageToSample <- 1

    outputFilePath <- 
        determineSampledTextFileSize(outputTextFileDirectory,
                                     percentageToSample)
    load(outputFilePath)

    languageId <- basename(outputTextFileDirectory)
    
    samplingStr <- initializeSamplingString(percentageToSample)
    
    sampledTextFileRegex <- 
        initializeSampledTextFileRegex(percentageToSample)

    lineCorpusFile <- file.path(outputTextFileDirectory,
                                pattern=paste0(languageId,
                                               samplingStr,
                                               "LineCorpus.RData"))

    unlink(lineCorpusFile)
    lineCorpus <- list()  
    for (curTextFile in dir(outputTextFileDirectory,
                            pattern=sampledTextFileRegex)) {
        lineCorpus[[curTextFile]] <- 
            formLineCorpus(outputTextFileDirectory,
                           curTextFile,
                           "english",
                           sampledDocNumLines,
                           blackList,
                           TRUE)
    }
    save(file=lineCorpusFile, lineCorpus)    
}
```

```{r loadData}
outputTextFileDirectory <- "./OutputData//en_US/"

percentageToSample <- 1
samplingStr <- initializeSamplingString(percentageToSample)

languageId <- basename(outputTextFileDirectory)

lineCorpusFile <- file.path(outputTextFileDirectory,
                            pattern=paste0(languageId,
                                           samplingStr,
                                           "LineCorpus.RData"))

load(lineCorpusFile)

sampledTextFiles <- names(lineCorpus)
for (n in seq_len(length(sampledTextFiles))) {
    if (n == 1){
        combinedCorpus <- lineCorpus[[sampledTextFiles[n]]]
    } else {
        combinedCorpus <- c(combinedCorpus,
                            lineCorpus[[sampledTextFiles[n]]])
    }
}
rm(lineCorpus)

tdm <- as.matrix(TermDocumentMatrix(combinedCorpus))
termFreqs <- sort(rowSums(tdm), decreasing=TRUE)
termPDF <- termFreqs / sum(termFreqs)
termCDF <- cumsum(termPDF)
save(file="./termCDF.RData",termCDF)

vocabulary <- sort(names(termCDF[1:which(termCDF >= 0.8)[1]]))
vocabularySize <- length(vocabulary)
termCDF <- termCDF[1:vocabularySize]
save(file="./vocabulary.RData",vocabulary,termCDF)
```

```{r tokenizeTrigrams}
trigramTDMFile <- "./triTdm.RData"
if (!file.exists(trigramTDMFile)) {
    TrigramTokenizer <- function(x) {
        RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))
    }
    
    for (n in seq(1,length(combinedCorpus))) {
        if (n %% 10 == 0) {
            print(sprintf("line #%d", n))    
        }
        
        lineCorpus <- Corpus(VectorSource(as.character(combinedCorpus[[n]])))
    
        curTdm <- TermDocumentMatrix(lineCorpus,
                                     control = list(tokenize = TrigramTokenizer))
        
        if (n == 1) {
            triTdm <- curTdm
        } else {
            triTdm <- c(triTdm,curTdm)
        }
    }
    triTdm <- sort(rowSums(as.matrix(triTdm)), decreasing=TRUE)
    save(file=trigramTDMFile,triTdm)
}
load(trigramTDMFile)
```

```{r initializeCommonTrigrams}
trigrams <- names(triTdm)
commonIdx <- numeric()

for (n in seq_len(length(trigrams))) {
    if (n %% 100 == 0) {
        print(sprintf("Trigram #%d (Out of %d)", n, length(trigrams)))    
    }
    
    curWords <- unlist(str_split(trigrams[n]," "))
    
    if (sum(curWords %in% vocabulary) == 3) {
        commonIdx <- append(commonIdx,n)
    }
}
save(file="./prediction.RData",list=ls())
```

```{r predictor}
load("./prediction.RData")

vocabularySize <- length(vocabulary)

transitionMatrix = matrix(numeric(vocabularySize^2),
                                  byrow=TRUE,
                                  nrow=vocabularySize,
                                  dimnames=list(vocabulary,
                                                vocabulary))

commonTriTdm <- triTdm[commonIdx]

for (m in seq_len(length(commonTriTdm))) {
    if (m %% 100 == 0) {
        print(sprintf("Trigram #%d (Out of %d)",m,length(commonTriTdm)))
    }
    
    curWords <- unlist(str_split(names(commonTriTdm[m])," "))
    
    for (n in seq(2,3)) {
        rowIdx <- which(grepl(paste0("^",curWords[n-1],"$"), vocabulary))
        colIdx <- which(grepl(paste0("^",curWords[n],"$"), vocabulary))
        transitionMatrix[rowIdx,colIdx] <- commonTriTdm[m]
    }
}

minProbability <- 0.01/(vocabularySize-1)

for (m in seq_len(nrow(transitionMatrix))) {
    curRowSum <- sum(transitionMatrix[m,])
    
    if (curRowSum > 0) {
        transitionMatrix[m,] = transitionMatrix[m,] / curRowSum
    } else {
        transitionMatrix[m,m] = 0.99
        
        n <- seq_len(ncol(transitionMatrix))
        n <- n[n != m]
        transitionMatrix[m,n] <- minProbability
    }
}
save(file="./transitionMatrix.RData",transitionMatrix)

textPredictor <- new("markovchain",
                     transitionMatrix=transitionMatrix)

commonTriTdm <- triTdm[commonIdx]

for (m in seq_len(length(commonTriTdm))) {
    print("-------------------------------------------")
    print(commonTriTdm[m])
    
    curPhrase <- unlist(str_split(names(commonTriTdm[m])," "))
    curState <- curPhrase[1]
    
    for (n in seq(1,2)) {
        curCondProb <- conditionalDistribution(textPredictor,
                                               curState)
        
        if (n == 1) {
            maxIdx <- grep(paste0("^",curPhrase[n+1],"$"),vocabulary)
        }else {
            maxIdx <- which.max(curCondProb)
        }
        
        nextState <- names(curCondProb[maxIdx])
        print(sprintf("%s -> %s (P: %g)",curState,
                                         nextState,
                                         curCondProb[maxIdx]))
        curState <- nextState
    } 
}
```

```{r}
load(file="./transitionMatrix.RData")
load(file="./vocabulary.RData")
load("./prediction.RData")
testIdx <- seq_len(length(trigrams))
testIdx <- testIdx[!testIdx %in% commonIdx]

textPredictor <- new("markovchain",
                     transitionMatrix=transitionMatrix)

vocabulary <- rownames(transitionMatrix)

set.seed(1234)
inputPhrase <- unlist(str_split(trigrams[commonIdx[1]]," "))
inputPhrase <- inputPhrase[1:2]

predictNextWord(inputPhrase, textPredictor)

predictNextWord <- function(curPhrase,
                            textPredictor) {
    textPrediction <- list()
    textPrediction$stateHistory <- character()
    
    numberWords <- length(curPhrase)
    curState <- curPhrase[1]
    vocabulary <- states(textPredictor)
    
    if (!curState %in% vocabulary) {
        randomIdx <- floor(length(vocabulary) * runif(1)) + 1
        curState <- vocabulary[randomIdx]
    }

    textPrediction$stateHistory <- 
            append(textPrediction$stateHistory, curState)
    
    for (n in seq(2,numberWords)) {
        nextState <- curPhrase[n]
        if (!nextState %in% vocabulary) {
            curConditionalProbability <- 
                conditionalDistribution(textPredictor, curState)
            
            nextState <- names(which.max(curConditionalProbability))
        }
        curState <- nextState
     
        textPrediction$stateHistory <- 
            append(textPrediction$stateHistory, curState)
    }

    textPrediction$conditionalProbability <- 
        sort(conditionalDistribution(textPredictor, curState),
             decreasing=TRUE)[1:4]
    
    return(textPrediction)
}


```
