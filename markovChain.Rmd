---
title: "MarkovChain"
output: html_document
---

```{r setupEnvironment}
library(ggplot2)
library(Gmisc)
library(rJava)
library(RWeka)
library(R.utils)
library(stringi)
library(stringr)
library(textcat)
library(tm)
library(xtable)
library(markovchain)

source("./determineTextFileSize.R")
source("./determineWordCount.R")
source("./formLineCorpus.R")
source("./sampleTextFile.R")
```

```{r updateLineCorpus}
updateLineCorpus <- TRUE
if (updateLineCorpus)
    blackList <- read.csv("./Data/Terms-to-Block.csv",header=FALSE,skip=4)
    blackList <- blackList[,2]
    blackList <- gsub(",","",blackList)
    blackList <- gsub(" ","",blackList)
    blackList <- gsub("[0-9]+","",blackList)
    blackList <- gsub("[\\.-]","",blackList)
    blackList <- blackList[!grepl("^a$",blackList)]
    blackList <- unique(blackList[blackList != ""])

    outputTextFileDirectory <- "./OutputData//en_US/"
    percentageToSample <- 1

    outputFilePath <- 
        determineSampledTextFileSize(outputTextFileDirectory,
                                     percentageToSample)
    load(outputFilePath)

    languageId <- basename(outputTextFileDirectory)
    
    samplingStr <- initializeSamplingString(percentageToSample)
    
    sampledTextFileRegex <- 
        initializeSampledTextFileRegex(percentageToSample)

    lineCorpusFile <- file.path(outputTextFileDirectory,
                                pattern=paste0(languageId,
                                               samplingStr,
                                               "LineCorpus.RData"))

    unlink(lineCorpusFile)
    lineCorpus <- list()  
    for (curTextFile in dir(outputTextFileDirectory,
                            pattern=sampledTextFileRegex)) {
        lineCorpus[[curTextFile]] <- 
            formLineCorpus(outputTextFileDirectory,
                           curTextFile,
                           "english",
                           sampledDocNumLines,
                           blackList,
                           TRUE)
    }
    save(file=lineCorpusFile, lineCorpus)    
}
```

```{r testProcessChunk}
textFilePath <- "./OutputData//en_US/"
textFile <- "en_US.blogs1p00Percent.txt"
lines_to_read <- 100
h_conn <- file(file.path(textFilePath, textFile), "r", blocking=FALSE)
cur_chunk <- readLines(h_conn, lines_to_read, skipNul=TRUE)
close(h_conn)

source("./formLineCorpus.R")
c <- processDocumentChunk(c, blackList)
tdm <- TermDocumentMatrix(c)
```

```{r loadData}
outputTextFileDirectory <- "./OutputData//en_US/"

percentageToSample <- 1
samplingStr <- initializeSamplingString(percentageToSample)

languageId <- basename(outputTextFileDirectory)

lineCorpusFile <- file.path(outputTextFileDirectory,
                            pattern=paste0(languageId,
                                           samplingStr,
                                           "LineCorpus.RData"))

load(lineCorpusFile)

sampledTextFiles <- names(lineCorpus)
for (n in seq_len(length(sampledTextFiles))) {
    if (n == 1){
        combinedCorpus <- lineCorpus[[sampledTextFiles[n]]]
    } else {
        combinedCorpus <- c(combinedCorpus,
                            lineCorpus[[sampledTextFiles[n]]])
    }
}
rm(lineCorpus)

tdm <- as.matrix(TermDocumentMatrix(combinedCorpus))
termFreqs <- sort(rowSums(tdm), decreasing=TRUE)
termPDF <- termFreqs / sum(termFreqs)
termCDF <- cumsum(termPDF)

vocabulary <- names(termCDF[1:which(termCDF >= 0.75)[1]])
vocabulary <- sort(vocabulary[grep("^[^0-9]+$",vocabulary)])
```

```{r tokenizeTrigrams}
trigramTDMFile <- "./triTdm.RData"
if (!file.exists(trigramTDMFile)) {
    TrigramTokenizer <- function(x) {
        RWeka::NGramTokenizer(x, RWeka::Weka_control(min = 3, max = 3))
    }
    
    for (n in seq(1,length(combinedCorpus))) {
        if (n %% 10 == 0) {
            print(sprintf("line #%d", n))    
        }
        
        lineCorpus <- Corpus(VectorSource(as.character(combinedCorpus[[n]])))
    
        curTdm <- TermDocumentMatrix(lineCorpus,
                                     control = list(tokenize = TrigramTokenizer))
        
        if (n == 1) {
            triTdm <- curTdm
        } else {
            triTdm <- c(triTdm,curTdm)
        }
    }
    triTdm <- sort(rowSums(as.matrix(triTdm)), decreasing=TRUE)
    save(file=trigramTDMFile,
         triTdm,
         termPDF,
         termCDF,
         vocabulary)
}
load(trigramTDMFile)
```

```{r}
trigrams <- names(triTdm)
commonIdx <- numeric()

for (n in seq_len(length(trigrams))) {
    if (n %% 100 == 0) {
        print(sprintf("Trigram #%d (Out of %d)", n, length(trigrams)))    
    }
    
    curWords <- unlist(str_split(trigrams[n]," "))
    
    if (sum(curWords %in% vocabulary) == 3) {
        commonIdx <- append(commonIdx,n)
    }
}
save(file="./prediction.RData",list=ls())
```

```{r predictor}
load("./prediction.RData")

vocabularySize <- length(vocabulary)

transitionMatrix = matrix(numeric(vocabularySize^2),
                                  byrow=TRUE,
                                  nrow=vocabularySize,
                                  dimnames=list(vocabulary,
                                                vocabulary))

commonTriTdm <- triTdm[commonIdx]

for (m in seq_len(length(commonTriTdm))) {
    if (m %% 100 == 0) {
        print(sprintf("Trigram #%d (Out of %d)",m,length(commonTriTdm)))
    }
    
    curWords <- unlist(str_split(names(commonTriTdm[m])," "))
    
    for (n in seq(2,3)) {
        rowIdx <- which(grepl(paste0("^",curWords[n-1],"$"), vocabulary))
        colIdx <- which(grepl(paste0("^",curWords[n],"$"), vocabulary))
        transitionMatrix[rowIdx,colIdx] <- commonTriTdm[m]
    }
}

minProbability <- 0.01/(vocabularySize-1)

for (m in seq_len(nrow(transitionMatrix))) {
    curRowSum <- sum(transitionMatrix[m,])
    
    if (curRowSum > 0) {
        transitionMatrix[m,] = transitionMatrix[m,] / curRowSum
    } else {
        transitionMatrix[m,m] = 0.99
        
        n <- seq_len(ncol(transitionMatrix))
        n <- n[n != m]
        transitionMatrix[m,n] <- minProbability
    }
}

textPredictor <- new("markovchain",
                     transitionMatrix=transitionMatrix)

commonTriTdm <- triTdm[commonIdx]

for (m in seq_len(length(triTdm))) {
    print("-------------------------------------------")
    print(commonTriTdm)
    
    curPhrase <- unlist(str_split(names(commonTriTdm[m])," "))
    curState <- curPhrase[1]
    
    for (n in seq(1,2)) {
        curCondProb <- conditionalDistribution(textPredictor,
                                               curState)
        
        if (n == 1) {
            maxIdx <- grep(paste0("^",curPhrase[n+1],"$"),vocabulary)
        }else {
            maxIdx <- which.max(curCondProb)    
        }
        
        nextState <- names(curCondProb[maxIdx])
        print(sprintf("%s -> %s (P: %g)",curState,
                                         nextState,
                                         curCondProb[maxIdx]))
        curState <- nextState
    } 
}
```

```{r predictor0}
load("./prediction.RData")
triTdm0 <- head(triTdm,n=50)

predictorVocabulary <- sort(unique(unlist(str_split(paste(names(triTdm0),
                                                          collapse=" "),
                                                    " "))))

predictorVocabularySize <- length(predictorVocabulary)

transitionMatrix = matrix(numeric(predictorVocabularySize^2),
                                  byrow=TRUE,
                                  nrow=predictorVocabularySize,
                                  dimnames=list(predictorVocabulary,
                                                predictorVocabulary))

for (m in seq_len(length(triTdm0))) {
    curWords <- unlist(str_split(names(triTdm0[m])," "))
    
    for (n in seq(2,3)) {
        rowIdx <- which(grepl(paste0("^",curWords[n-1],"$"),
                              predictorVocabulary))
        colIdx <- which(grepl(paste0("^",curWords[n],"$"),
                                     predictorVocabulary))
        transitionMatrix[rowIdx,colIdx] <- triTdm0[m]
    }
}

for (m in seq_len(nrow(transitionMatrix))) {
    curRowSum <- sum(transitionMatrix[m,])
    
    if (curRowSum > 0) {
        transitionMatrix[m,] = transitionMatrix[m,] / curRowSum
    } else {
        transitionMatrix[m,m] = 1
    }
}

textPredictor <- new("markovchain",
                     transitionMatrix=transitionMatrix)

for (m in seq_len(length(triTdm0))) {
    print("-------------------------------------------")
    print(triTdm0[m])
    
    curState <- unlist(str_split(names(triTdm0[m])," "))[1]
    for (n in seq(1,2)) {
        curCondProb <- conditionalDistribution(textPredictor,
                                               curState)
        maxIdx <- which.max(curCondProb)
        nextState <- names(curCondProb[maxIdx])
        print(sprintf("%s -> %s (P: %g)",curState,
                                         nextState,
                                         curCondProb[maxIdx]))
        curState <- nextState
    }    
}
```